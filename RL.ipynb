{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMICf5D9L1D9HwJUKR1PDwZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/polyjuice06/machine_learning/blob/main/RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "DaIbcq6p_Qni"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "LEFT = 0\n",
        "DOWN = 1\n",
        "RIGHT = 2\n",
        "UP = 3\n",
        "\n",
        "MAP = [\n",
        "        \"SFFF\",\n",
        "        \"FHFH\",\n",
        "        \"FFFH\",\n",
        "        \"HFFG\"\n",
        "    ]\n",
        "\n",
        "class FrozenLakeEnv():\n",
        "\n",
        "    \"\"\"\n",
        "    Winter is here. You and your friends were tossing around a frisbee at the park\n",
        "    when you made a wild throw that left the frisbee out in the middle of the lake.\n",
        "    The water is mostly frozen, but there are a few holes where the ice has melted.\n",
        "    If you step into one of those holes, you'll fall into the freezing water.\n",
        "    The surface is described using a grid like the following\n",
        "\n",
        "        SFFF\n",
        "        FHFH\n",
        "        FFFH\n",
        "        HFFG\n",
        "\n",
        "    S : starting point, safe\n",
        "    F : frozen surface, safe\n",
        "    H : hole, fall to your doom\n",
        "    G : goal, where the frisbee is located\n",
        "    The episode ends when you reach the goal or fall in a hole.\n",
        "    You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, is_slippery):\n",
        "\n",
        "        self.map = np.asarray(MAP, dtype='c')\n",
        "        nrow, ncol = 4, 4\n",
        "        self.nA = 4\n",
        "        self.nS = nrow * ncol\n",
        "\n",
        "        def to_s(row, col):\n",
        "            return row * ncol + col\n",
        "\n",
        "        def move(row, col, a):\n",
        "            if a == 0:  # left\n",
        "                col = max(col - 1, 0)\n",
        "            elif a == 1:  # down\n",
        "                row = min(row + 1, nrow - 1)\n",
        "            elif a == 2:  # right\n",
        "                col = min(col + 1, ncol - 1)\n",
        "            elif a == 3:  # up\n",
        "                row = max(row - 1, 0)\n",
        "            return (row, col)\n",
        "\n",
        "        mdp = list( )\n",
        "        for i in range(self.nS):\n",
        "            mdp.append([[], [], [], []])\n",
        "\n",
        "        for row in range(nrow):\n",
        "            for col in range(ncol):\n",
        "                s = to_s(row, col)\n",
        "                for a in range(4):\n",
        "                    letter = self.map[row, col]\n",
        "                    if letter in b'GH':\n",
        "                        mdp[s][a].append([1.0, s, 0])\n",
        "                    else:\n",
        "                        if is_slippery:\n",
        "                            for b in [(a - 1) % 4, a, (a + 1) % 4]:\n",
        "                                newrow, newcol = move(row, col, b)\n",
        "                                newstate = to_s(newrow, newcol)\n",
        "                                newletter = self.map[newrow, newcol]\n",
        "                                rew = float(newletter == b'G')\n",
        "                                mdp[s][a].append((1.0 / 3.0, newstate, rew))\n",
        "                        else:\n",
        "                            newrow, newcol = move(row, col, a)\n",
        "                            newstate = to_s(newrow, newcol)\n",
        "                            newletter = self.map[newrow][newcol]\n",
        "                            rew = float(newletter == b'G')\n",
        "                            mdp[s][a].append([1.0, newstate, rew])\n",
        "\n",
        "        self.MDP = mdp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 교수님 구현\n",
        "import numpy as np\n",
        "\n",
        "def policy_iteration(env, gamma=0.99, theta=1e-8):\n",
        "    V = np.zeros(env.nS)\n",
        "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
        "    policy_stable = True\n",
        "    \n",
        "    while policy_stable:\n",
        "        # Policy Evaluation\n",
        "        while True:\n",
        "            delta = 0\n",
        "            new_V = np.zeros(env.nS)\n",
        "            q = np.zeros([env.nS, env.nA]) / env.nA\n",
        "            for s in range(env.nS):\n",
        "                # 아래의 For문으로 각 action에 대한 q 값을 구함\n",
        "                for a in range(env.nA):\n",
        "                    for i in range(len(env.MDP[s][a])):\n",
        "                        x = env.MDP[s][a][i]\n",
        "                        q[s][a] += x[0] * (x[2] + gamma * V[x[1]])\n",
        "                # deterministic, greedy한 action에 해당하는 q값이 new_V[s]가 됨\n",
        "                new_V[s] = q[s][np.argmax(policy[s])]\n",
        "            # 모든 s에 대해서 V 업데이트 완료, 업데이트 전/후 차이가 theta보다 작으면 수렴으로 판단\n",
        "            difference = max(np.abs(new_V - V))\n",
        "            delta = max(delta, difference)\n",
        "            V = new_V\n",
        "            if delta < theta:\n",
        "                break\n",
        "        # Policy Improvement\n",
        "        differ = np.zeros(env.nS) # policy update가 되었는지 판단하기 위한 array\n",
        "        for s in range(env.nS):\n",
        "            old_action = np.argmax(policy[s])\n",
        "            policy[s] = np.zeros(env.nA)\n",
        "            new_action = np.argmax(q[s])\n",
        "            policy[s][new_action] = 1 # policy update\n",
        "            # update가 되었으면 False, 그대로이면 True\n",
        "            if old_action == new_action:\n",
        "                differ[s] = True\n",
        "            else:\n",
        "                differ[s] = False\n",
        "        # 모든 state에서 update가 안 되었다면 (더 이상의 Update가 없다면) break\n",
        "        # 아직 Update가 된다면 다시 Policy Evaluation으로 \n",
        "        if np.all(differ) == True:\n",
        "            break\n",
        "\n",
        "    return policy, V\n",
        "\n",
        "def value_iteration(env, gamma=0.99, theta=1e-8):\n",
        "    V = np.zeros(env.nS)\n",
        "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
        "    while True:\n",
        "        delta = 0\n",
        "        new_V = np.zeros(env.nS)\n",
        "        for s in range(env.nS):\n",
        "            policy[s] = np.zeros(env.nA) \n",
        "            q = np.zeros(env.nA)\n",
        "            # state s에서, 4가지의 모든 action에 대해서 Q값을 구함\n",
        "            for a in range(env.nA):\n",
        "                for i in range(len(env.MDP[s][a])):\n",
        "                    x = env.MDP[s][a][i]\n",
        "                    q[a] += x[0] * (x[2] + gamma * V[x[1]])\n",
        "            # new_V[s]를 max(q)로 update하며, argmax(q)가 action이 되고 policy도 update를 함\n",
        "            new_V[s] = max(q)\n",
        "            action = np.argmax(q)\n",
        "            policy[s] = np.zeros(env.nA)\n",
        "            policy[s][action] = 1\n",
        "        # 모든 state에 대해서 for loop를 다 돌았다면 이러한 반복을 언제 멈출지 결정해야 함\n",
        "        # 기존의 V와 update한 new_V 배열의 차이 중 가장 큰 값이 delta보다 크면 delta 업데이트\n",
        "        # 그리고 기존의 V를 업데이트한 new_V로 넣어줌\n",
        "        # 차이 delta가 theta보다 작다면 반복문 break\n",
        "        difference = max(np.abs(V - new_V))\n",
        "        delta = max(delta, difference)\n",
        "        V = new_V\n",
        "        if delta < theta:\n",
        "            break\n",
        "    return policy, V"
      ],
      "metadata": {
        "id": "1iro0FUfvu8c"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    mode = int(input(\"1.Not Slippery, 2.Slippery: \"))\n",
        "    if mode == 1:\n",
        "        env = FrozenLakeEnv(is_slippery=False)\n",
        "        break\n",
        "    elif mode == 2:\n",
        "        env = FrozenLakeEnv(is_slippery=True)\n",
        "        break\n",
        "    else:\n",
        "        print(\"잘못 입력했습니다.\")\n",
        "\n",
        "# 환경 state 개수 및 action 개수\n",
        "#print(env.nS, env.nA)\n",
        "\n",
        "while True:\n",
        "    mode = int(input(\"1.Policy Iteration, 2.Value Iteration: \"))\n",
        "    if mode == 1:\n",
        "        policy, V = policy_iteration(env)\n",
        "        break\n",
        "    elif mode == 2:\n",
        "        policy, V = value_iteration(env)\n",
        "        break\n",
        "    else:\n",
        "        print(\"잘못 입력했습니다.\")\n",
        "print()\n",
        "\n",
        "# print( env.MDP[0][1] )\n",
        "# state 0 에서 action 1 을 선택했을 때 [상태 이동 확률, 도착 state, reward]\n",
        "\n",
        "print(\"Optimal State-Value Function:\")\n",
        "for i in range(len(V)):\n",
        "    if i>0 and i%4==0:\n",
        "        print()\n",
        "    print('{0:0.3f}'.format(V[i]), end=\"\\t\")\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"Optimal Policy [LEFT, DOWN, RIGHT, UP]:\")\n",
        "action = {0:\"LEFT\", 1:\"DOWN\", 2:\"RIGHT\", 3:\"UP\"}\n",
        "for i in range(len(policy)):\n",
        "    if i>0 and i%4==0:\n",
        "        print()\n",
        "    print(policy[i], end='    ')\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kt_mExzX2nH",
        "outputId": "6cf0d764-f9dc-44d1-dbbe-9ebd4cf89794"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.Not Slippery, 2.Slippery: 1\n",
            "1.Policy Iteration, 2.Value Iteration: 1\n",
            "\n",
            "Optimal State-Value Function:\n",
            "0.951\t0.961\t0.970\t0.961\t\n",
            "0.961\t0.000\t0.980\t0.000\t\n",
            "0.970\t0.980\t0.990\t0.000\t\n",
            "0.000\t0.990\t1.000\t0.000\t\n",
            "\n",
            "Optimal Policy [LEFT, DOWN, RIGHT, UP]:\n",
            "[0. 1. 0. 0.]    [0. 0. 1. 0.]    [0. 1. 0. 0.]    [1. 0. 0. 0.]    \n",
            "[0. 1. 0. 0.]    [1. 0. 0. 0.]    [0. 1. 0. 0.]    [1. 0. 0. 0.]    \n",
            "[0. 0. 1. 0.]    [0. 1. 0. 0.]    [0. 1. 0. 0.]    [1. 0. 0. 0.]    \n",
            "[1. 0. 0. 0.]    [0. 0. 1. 0.]    [0. 0. 1. 0.]    [1. 0. 0. 0.]    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#내 구현\n",
        "import numpy as np\n",
        "def policy_iteration(env, gamma=0.99, theta=1e-8):\n",
        "    V = np.zeros(env.nS,env.vA)\n",
        "    policy = np.zeros(env.nS,env.vA,dtype = 'int')\n",
        "    x = 0\n",
        "    flag = True\n",
        "    while flag and x < 100:\n",
        "      V1,y,error=np.zeros(env.nS),0,1\n",
        "      while error > theta and y < 100:\n",
        "        new_value_function = np.zeros(env.nS)\n",
        "        for i in range(env.nS):\n",
        "          a = policy[i]\n",
        "          transitions = env.MDP[i][a]\n",
        "          for transition in transitions:\n",
        "            prob, nextS, reward = transition\n",
        "            new_value_function[i] += prob*(reward + gamma*V1[nextS])\n",
        "        error = np.max(np.abs(new_value_function - V1))\n",
        "        V1 = new_value_function\n",
        "      V = V1\n",
        "      new_policy = np.zeros(env.nS, dtype='int')\n",
        "      for state in range(env.nS):\n",
        "        Qs = np.zeros(env.nA)\n",
        "        for a in range(env.nA):\n",
        "          transitions = env.MDP[state][a]\n",
        "          for transition in transitions:\n",
        "            prob, nextS, reward = transition\n",
        "            Qs[a] += prob*(reward + gamma*V[nextS])\n",
        "        max_as = np.where(Qs==Qs.max())\n",
        "        max_as = max_as[0]\n",
        "        new_policy[state] = max_as[0]\n",
        "      diff_policy = new_policy-policy\n",
        "      if np.linalg.norm(diff_policy)==0: flag = False\n",
        "      policy = new_policy\n",
        "      x+=1\n",
        "    return policy,V\n",
        "\n",
        "def value_iteration(env, gamma=0.99, theta=1e-8):\n",
        "    V = np.zeros(env.nS)\n",
        "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
        "    error = 1\n",
        "    while error > theta:\n",
        "      new_value_function = np.zeros(env.nS)\n",
        "      for s in range(env.nS):\n",
        "        Qs = np.zeros(env.nA)\n",
        "        for a in range(env.nA):\n",
        "          transitions = env.MDP[s][a]\n",
        "          for transition in transitions:\n",
        "            prob, nextS, reward = transition\n",
        "            Qs[a] += prob*(reward + gamma*V[nextS])\n",
        "        new_value_function[s] = max(Qs)\n",
        "      diff_vf = new_value_function-V\n",
        "      V = new_value_function\n",
        "      error = np.linalg.norm(diff_vf)\n",
        "\n",
        "    for s in range(env.nS):\n",
        "      Qs = np.zeros(env.nA)\n",
        "      for a in range(env.nA):\n",
        "        transitions = env.MDP[s][a]\n",
        "        for transition in transitions:\n",
        "          prob, nextS, reward = transition\n",
        "          Qs[a] += prob*(reward + gamma*V[nextS])\n",
        "        max_as = np.where(Qs==Qs.max())\n",
        "        max_as = max_as[0]\n",
        "      policy[s] = max_as[0]\n",
        "    return policy, V"
      ],
      "metadata": {
        "id": "-bU-KoKDv5fS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}